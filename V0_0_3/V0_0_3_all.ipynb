{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "%matplotlib inline\nimport torch as t\nfrom torchvision import transforms \nimport random\n\n\ndef image_transforms(size=224):\n    fr_transforms = transforms.Compose([\n                        transforms.Resize(size),\n                        transforms.RandomCrop(224),\n                        transforms.RandomHorizontalFlip(),\n                        transforms.RandomVerticalFlip(),\n                        transforms.RandomRotation(45),\n                        transforms.ColorJitter(brightness=0.05, contrast=0.1, saturation=0.3, hue=0.2),\n                        transforms.ToTensor(),\n                        transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n                    ])\n    return fr_transforms\n\ndef enhance_transforms():\n    possibility = random.randint(0, 19)\n    if possibility % 20 ==0:#0.5%\u6982\u7387\u4e0d\u53d1\u751f\u53d8\u5316,\u76f8\u5f53\u4e8e\u6570\u636e\u96c6\u6269\u5bb920\u500d\n        output_transforms = transforms.Compose([\n                                    transforms.Resize(224),\n                                    transforms.CenterCrop(224),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))])\n        return output_transforms\n    else:\n        possibility2 = random.randint(0, 3)\n        if possibility2%5==0:#20%\u6982\u7387\u53d1\u751f\u4e0d\u540c\u6bd4\u4f8b\u653e\u7f29\n            return image_transforms(224)\n        elif possibility2%5==1:\n            return image_transforms(245)#\u9762\u79ef\u653e\u59271.2\u500d\n        elif possibility2%5==2:\n            return image_transforms(274)#\u9762\u79ef\u653e\u59271.5\u500d\n        elif possibility2%5==3:\n            return image_transforms(316)#\u9762\u79ef\u653e\u59272\u500d\n    \n\ndef transform_standard():\n    output_transforms = transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n        ])\n    return output_transforms\n    ", "execution_count": 1, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import torch.nn as nn\n\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n \n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=False)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n \n    def forward(self, x):\n        residual = x\n        out = self.relu(x)\n \n        out = self.conv1(out)\n        out = self.bn1(out)\n        out = self.relu(out)\n \n        out = self.conv2(out)\n        out = self.bn2(out)\n \n        if self.downsample is not None:\n            residual = self.downsample(x)\n \n        out = out + residual\n        out = self.relu(out)\n \n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n \n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=False)\n        self.downsample = downsample\n        self.stride = stride\n \n\n    def forward(self, x):\n        residual = x\n \n        out = self.relu(x)\n        out = self.conv1(out)\n        out = self.bn1(out)\n        out = self.relu(out)\n \n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n \n        out = self.conv3(out)\n        out = self.bn3(out)\n \n        if self.downsample is not None:\n            residual = self.downsample(x)\n \n        out = out + residual\n        out = self.relu(out)\n \n        return out", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import re\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport moxing as mox\nmox.file.shift('os', 'mox')\n\nclass DataClassify(Dataset):\n    def __init__(self, root, transforms=None, mode=None):\n        #\u5b58\u653e\u56fe\u50cf\u5730\u5740\n        self.imgs = [x.path for x in mox.file.scan_dir(root) if\n            x.name.endswith(\".jpg\")]\n        self.labels = [y.path for y in mox.file.scan_dir(root) if\n            y.name.endswith(\".txt\")]\n        self.transforms = transforms\n        \n    def __getitem__(self, index):\n        #\u8bfb\u53d6\u56fe\u50cf\u6570\u636e\u5e76\u8fd4\u56de\n        img_path = self.imgs[index]\n        label = int(re.sub('\\D','',mox.file.read(self.labels[index])[-4:]))\n        data = Image.open(img_path)\n        if self.transforms:\n            data = self.transforms(data)\n        return data, label\n    \n    def __len__(self):\n        return len(self.imgs)\n\n\ndef ListToTensor(lister):\n    nparr = np.asarray(lister)\n    tens = t.from_numpy(nparr)\n    return tens", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.13.0-de803ac9\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#coefficient = 5#\u8d85\u53c2\u6570\uff0c\u5b66\u4e60\u7387\u8870\u51cf\u7cfb\u6570\uff0c\u4e0e\u6a21\u578b\u6709\u5173\uff0c\u9700\u8981\u8c03\u53c2\n\ndef dloss(loss_list, lr=0.001, coefficient=5, init_lr=0.001):\n\tif len(loss_list) <=2:\n\t\treturn init_lr#\u8d85\u53c2\u6570,\u521d\u59cb\u5b66\u4e60\u7387\uff0c\u4e0e\u8bad\u7ec3\u6837\u672c\u96c6\u6709\u5173\uff0c\u4e0e\u6a21\u578b\u5173\u7cfb\u8f83\u5c0f\uff0c\u786e\u5b9a\u8bad\u7ec3\u96c6\u540e\u5373\u53ef\u56fa\u5b9a\n\tif (loss_list[-2] - loss_list[-1])/loss_list[-2] > 0.02:\n\t\treturn init_lr\n\treturn lr/coefficient\n", "execution_count": 4, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport numpy as np\nimport moxing as mox\nmox.file.shift('os', 'mox')\n\ndef curve_draw(train_loss_list, train_accurate_list, test_accurate_list, log_dir, version):\n    x1 = range(0, len(train_loss_list))\n    x2 = range(0, len(train_accurate_list))\n    x3 = range(0, len(test_accurate_list))\n    y1 = train_loss_list\n    y2 = train_accurate_list\n    y3 = test_accurate_list\n    plt.subplot(3, 1, 1)\n    plt.plot(x1, y1, 'o-')\n    plt.xlabel('epochs')\n    plt.subplot(3, 1, 2)\n    plt.plot(x2, y2, '*-')\n    plt.xlabel('epochs')\n    plt.subplot(3, 1, 3)\n    plt.plot(x3, y3, 'x-')\n    plt.xlabel('epochs')\n    #plt.savefig(log_dir + 'result' + version + '.png')\n    train_loss_list_path = log_dir + \"train_loss\" + version + '.csv'\n    train_accurate_list_path = log_dir + \"train_accurate\" + version + '.csv'\n    test_accurate_list_path = log_dir + \"test_accurate\" + version + '.csv'\n    train_loss_list_csv = pd.DataFrame(train_loss_list)\n    train_accurate_list_csv = pd.DataFrame(train_accurate_list)\n    test_accurate_list_csv = pd.DataFrame(test_accurate_list)\n\n    with mox.file.File(train_loss_list_path,\"w\") as c1:\n        train_loss_list_csv.to_csv(c1)\n    with mox.file.File(train_accurate_list_path,\"w\") as c2:\n        train_accurate_list_csv.to_csv(c2)\n    with mox.file.File(test_accurate_list_path,\"w\") as c3:\n        test_accurate_list_csv.to_csv(c3)\n", "execution_count": 5, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import torch.nn as nn\n\nclass ResNet(nn.Module):\n \n\tdef __init__(self, block, layers, num_classes=1000):\n\t\tself.inplanes = 64\n\t\tsuper(ResNet, self).__init__()\n\t\tself.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n\t\t\t\t\t\t\t   bias=False)\n\t\tself.bn1 = nn.BatchNorm2d(64)\n\t\tself.relu = nn.ReLU(inplace=False)\n\t\tself.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\t\tself.layer1 = self._make_layer(block, 64, layers[0])\n\t\tself.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n\t\tself.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n\t\tself.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\t\tself.avgpool = nn.AvgPool2d(7, stride=1)\n\t\tself.fc = nn.Linear(512 * block.expansion, num_classes)\n \n\t\tfor m in self.modules():\n\t\t\tif isinstance(m, nn.Conv2d):\n\t\t\t\tnn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n\t\t\telif isinstance(m, nn.BatchNorm2d):\n\t\t\t\tnn.init.constant_(m.weight, 1)\n\t\t\t\tnn.init.constant_(m.bias, 0)\n \n\tdef _make_layer(self, block, planes, blocks, stride=1):\n\t\tdownsample = None\n\t\tif stride != 1 or self.inplanes != planes * block.expansion:\n\t\t\tdownsample = nn.Sequential(\n\t\t\t\tnn.Conv2d(self.inplanes, planes * block.expansion,\n\t\t\t\t\t\t  kernel_size=1, stride=stride, bias=False),\n\t\t\t\tnn.BatchNorm2d(planes * block.expansion),\n\t\t\t)\n \n\t\tlayers = []\n\t\tlayers.append(block(self.inplanes, planes, stride, downsample))\n\t\tself.inplanes = planes * block.expansion\n\t\tfor i in range(1, blocks):\n\t\t\tlayers.append(block(self.inplanes, planes))\n \n\t\treturn nn.Sequential(*layers)\n \n\tdef forward(self, x):\n\t\tx = self.relu(x)\n\t\tx = self.conv1(x)\n\t\tx = self.bn1(x)\n\t\tx = self.relu(x)\n\t\tx = self.maxpool(x)\n \n\t\tx = self.layer1(x)\n\t\tx = self.layer2(x)\n\t\tx = self.layer3(x)\n\t\tx = self.layer4(x)\n \n\t\tx = self.avgpool(x)\n\t\tx = x.view(x.size(0), -1)\n\t\tx = self.fc(x)\n \n\t\treturn x\n\n\t\t\ndef resnet18(pretrained=False, **kwargs):\n\t\"\"\"Constructs a ResNet-18 model.\n\tArgs:\n\t\tpretrained (bool): If True, returns a model pre-trained on ImageNet\n\t\"\"\"\n\tmodel = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n\tif pretrained:\n\t\tmodel.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n\treturn model\n \n \ndef resnet34(pretrained=False, **kwargs):\n\t\"\"\"Constructs a ResNet-34 model.\n\tArgs:\n\t\tpretrained (bool): If True, returns a model pre-trained on ImageNet\n\t\"\"\"\n\tmodel = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n\tif pretrained:\n\t\tmodel.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n\treturn model\n \n \ndef resnet50(pretrained=False, **kwargs):\n\t\"\"\"Constructs a ResNet-50 model.\n\tArgs:\n\t\tpretrained (bool): If True, returns a model pre-trained on ImageNet\n\t\"\"\"\n\tmodel = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n\tif pretrained:\n\t\tmodel.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n\treturn model\n \n \ndef resnet101(pretrained=False, **kwargs):\n\t\"\"\"Constructs a ResNet-101 model.\n\tArgs:\n\t\tpretrained (bool): If True, returns a model pre-trained on ImageNet\n\t\"\"\"\n\tmodel = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n\tif pretrained:\n\t\tmodel.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n\treturn model\n \n \ndef resnet152(pretrained=False, **kwargs):\n\t\"\"\"Constructs a ResNet-152 model.\n\tArgs:\n\t\tpretrained (bool): If True, returns a model pre-trained on ImageNet\n\t\"\"\"\n\tmodel = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n\tif pretrained:\n\t\tmodel.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n\treturn model", "execution_count": 6, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import torch as t\nfrom torch.autograd import Variable\n\ndef evaluate(model, data_loader_test):\n\tmodel.eval()\n\tcorrect = 0.0\n\trun_correct = 0.0\n\tfor data in iter(data_loader_test):\n\t\tX_test, X_label = data\n\t\tlabel = []\n\t\tfor la in X_label:\n\t\t\tlabel.append(la)\n\t\tlabel = ListToTensor(label)\n\t\tX_test = Variable(X_test)\n\t\tX_label = Variable(label)\n\t\twith t.no_grad():\n\t\t\toutputs = model(X_test)\n\t\t\t_, pred = t.max(F.softmax(outputs, dim=1).data, 1)\n\t\t\trun_correct += (pred == X_label.data).sum()\n\t\t\tcorr = (1.*run_correct).item()\n\treturn corr", "execution_count": 7, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import torch as t\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\n\ndef train_once(data_loader_train, net, optimizer, cost):\n\trun_loss = 0.0\n\trun_correct = 0.0\n\n\tfor data in iter(data_loader_train):\n\t\tX_train, X_label = data\n\t\tlabel = []\n\t\tfor la in X_label:\n\t\t\tlabel.append(la)\n\t\tlabel = ListToTensor(label)\n\n\t\tX_train = Variable(X_train)\n\t\tX_label = Variable(label)\n\t\toptimizer.zero_grad()\n\t\toutputs = net(X_train)\n\n\t\t_, pred = t.max(F.softmax(outputs, dim=1).data, 1)\n\t\tloss = cost(outputs, X_label)\n\t\trun_loss += loss.data\n\t\trun_loss = run_loss.item()\n\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\trun_correct += (pred == X_label.data).sum()\n\t\tcorr = (1.*run_correct).item()\n\treturn run_loss, corr\n\n\ndef train(epochs=120,\n\t\tinit_lr=0.001,\n\t\tlr_coefficient=5,\n\t\tweight_decay = 1e-8,\n\t\tmodel_num=1,\n\t\tbatch_size=64,\n\t\ttrain_dir='s3://classifier-gar/train_try/',\n\t\ttest_dir='s3://classifier-gar/test_try/',\n\t\tlog_dir='s3://classifier-gar/log/',#\u7528\u4e4b\u524d\u8bb0\u7740\u5199\u9ed8\u8ba4\u8def\u5f84\n\t\tversion = 'V0_0_0'):\n\n\t#loading_data\n\tprint(\"data loading...\\n\")\n\ttransform = enhance_transforms()\n\ttransform_std = transform_standard()\n\ttrainset = DataClassify(train_dir, transforms=transform)\n\ttestset = DataClassify(test_dir, transforms=transform_std)\n\ttotal_train = len(trainset)\n\ttotal_test = len(testset)\n\tdata_loader_train = t.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n\tdata_loader_test = t.utils.data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=False)\n\tprint(\"data loading complete\\n\")\n\n\t##################################\n\t#TO DO\n\t##################################\n\tif model_num==0:#\u5199\u4e0d\u540c\u6a21\u578b\u7684\u5206\u652f\n\t\texit(0)\n\telse:\n\t\tnet = resnet18()\n\t##################################\n\n\tcost = t.nn.CrossEntropyLoss()\n\ttrain_loss_list = []\n\ttrain_accurate_list = []\n\ttest_loss_list = []\n\ttest_accurate_list = []\n\t\n\tfor epoch in range(epochs):\n\t\tprint(\"epoch \" + str(epoch+1) + \" start training...\\n\")\n\n\t\tnet.train()\n\n\t\tlearning_rate = dloss(train_loss_list, init_lr, lr_coefficient, init_lr)\n\t\toptimizer = t.optim.Adam(list(net.parameters()), lr=learning_rate, weight_decay=weight_decay)\n\n\t\trun_loss, corr = train_once(data_loader_train,net, optimizer, cost)\n\t\ttrain_loss_list.append(run_loss/total_train)\n\t\ttrain_accurate_list.append(corr/total_train)\n\n\t\tprint('epoch %d, training loss %.6f, training accuracy %.4f ------\\n' %(epoch+1, run_loss/total_train, corr/total_train))\n\t\tprint(\"epoch \" + str(epoch+1) + \" finish training\\n\")\n\t\tprint(\"-----------------------------------------------\\n\")\n\t\t\n\n\t\tprint(\"epoch \" + str(epoch+1) + \" start testing...\\n\")\n\t\t\n\t\tnet.eval()\n\t\ttest_corr = evaluate(net, data_loader_test)\n\t\ttest_accurate_list.append(test_corr/total_test)\n\t\tprint('epoch %d, testing accuracy %.4f ------\\n' %(epoch+1, test_corr/total_test))\n\t\tprint(\"epoch \" + str(epoch+1) + \" finish testing\\n\")\n\t\tprint(\"-----------------------------------------------\\n\")\n\n\t#torch.save(net.module, net_name)#\u4fdd\u5b58\u6a21\u578b\u5168\u90e8\u5185\u5bb9\uff0c\u7528\u4e8e\u8fdb\u884c\u6a21\u578b\u7684\u5bfc\u5165\u5bfc\u51fa\uff0cnet_name\u540e\u7f00\u4e3apkl\n\t#\u8fd9\u79cd\u65b9\u5f0f\u4fdd\u5b58\u7684\u6a21\u578b\u9700\u8981\u4f7f\u7528net = torch.load(net_name)\u8fdb\u884c\u52a0\u8f7d\n\n\t#torch.save(net.state_dict(), net_name_para)#\u53ea\u4fdd\u5b58\u53c2\u6570\uff0c\u7528\u4e8e\u8fdb\u884c\u6a21\u578b\u7684\u8fc1\u79fb\uff0cnet_name\u540e\u7f00\u4e3apkl\n\t#\u8fd9\u79cd\u65b9\u5f0f\u4fdd\u5b58\u7684\u6a21\u578b\u52a0\u8f7d\u65f6\u9700\u8981\u5b9a\u4e49\u7f51\u7edc\uff0c\u5e76\u4e14\u9700\u8981\u52a0\u8f7d\u7684\u53c2\u6570\u540d\u79f0\u4e0e\u4fdd\u5b58\u6a21\u578b\u4e2d\u4e00\u81f4\n\t#\u5e76\u901a\u8fc7net.load_state_dict(torch.load(net_name_para))\u8fdb\u884c\u52a0\u8f7d\n\n\tcurve_draw(train_loss_list, train_accurate_list, test_accurate_list, log_dir, version)\n\n\tprint(\"mission complete\")\n", "execution_count": 10, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train(epochs=2,\n\t\tinit_lr=0.001,\n\t\tlr_coefficient=5,\n\t\tweight_decay = 1e-8,\n\t\tmodel_num=1,\n\t\tbatch_size=64,\n\t\ttrain_dir='s3://obs-garbageclassification/obs-dataset/mini_trail/',\n\t\ttest_dir='s3://obs-garbageclassification/obs-dataset/mini_trail/',\n\t\tlog_dir='s3://obs-garbageclassification/training_log/V0_0_3/',\n\t\tversion = 'V0_0_3')", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "data loading...\n\ndata loading complete\n\nepoch 1 start training...\n\nepoch 1, training loss 0.112299, training accuracy 0.0000 ------\n\nepoch 1 finish training\n\n-----------------------------------------------\n\nepoch 1 start testing...\n\nepoch 1, testing accuracy 0.0476 ------\n\nepoch 1 finish testing\n\n-----------------------------------------------\n\nepoch 2 start training...\n\nepoch 2, training loss 0.085390, training accuracy 0.2222 ------\n\nepoch 2 finish training\n\n-----------------------------------------------\n\nepoch 2 start testing...\n\nepoch 2, testing accuracy 0.1270 ------\n\nepoch 2 finish testing\n\n-----------------------------------------------\n\nmission complete\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<Figure size 432x288 with 3 Axes>", "image/png": "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\n"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pytorch-1.0.0", "display_name": "Pytorch-1.0.0", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}